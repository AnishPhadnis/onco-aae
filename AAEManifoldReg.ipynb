{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('./mcf7.data.npy')\n",
    "# 166 bits fingerprint, 1 concentration float, 1 TGI float\n",
    "unique_fp = np.load('./mcf7.unique.fp.npy')\n",
    "# there is 6252 unique fingerprints and multiple experiments with each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "test_data, train_data = np.vsplit(data, [100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrain_batch_size = 512\n",
    "batch_size = 64\n",
    "latent_space = 4\n",
    "input_space = 166\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_prior(size=(batch_size, latent_space)):\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "def batch_gen(data, batch_size=batch_size):\n",
    "    max_index = data.shape[0]/batch_size\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(data)\n",
    "        for i in xrange(max_index):\n",
    "            yield np.hsplit(data[batch_size*i:batch_size*(i+1)], [-2, -1])\n",
    "\n",
    "def same_gen(unq_fp, n_examples=batch_size, n_different=1, mu=-5.82, std=1.68):\n",
    "    '''\n",
    "    Generator of same fingerprints with different concentraition\n",
    "    '''\n",
    "    \n",
    "    if n_examples % n_different: \n",
    "        raise ValueError('n_examples(%s) must be divisible by n_different(%s)' % (n_examples, n_different))\n",
    "    max_index = unq_fp.shape[0] / n_different\n",
    "    targets = np.zeros((n_examples, n_examples))\n",
    "    block_size = n_examples/n_different\n",
    "    for i in xrange(n_different):\n",
    "        '''blocks of ones for every block of equal fp's'''\n",
    "        targets[i*block_size:(i+1)*block_size, i*block_size:(i+1)*block_size] = 1.\n",
    "    targets = targets > 0\n",
    "    \n",
    "    while 1:\n",
    "        np.random.shuffle(unq_fp)\n",
    "        for i in xrange(max_index):\n",
    "            batch_conc = np.random.normal(mu, std, size=(n_examples, 1))\n",
    "            batch_fp = np.repeat(unq_fp[i*n_different:(i+1)*n_different], [block_size]*n_different, axis=0)\n",
    "            yield batch_fp, batch_conc, targets\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fingerprint_tensor = tf.placeholder(tf.float32, [None, input_space])\n",
    "prior_tensor = tf.placeholder(tf.float32, [None, latent_space])\n",
    "conc_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "tgi_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "targets_tensor = tf.placeholder(tf.bool, [None, None])\n",
    "\n",
    "visible_tensor = tf.concat(1, [fingerprint_tensor, conc_tensor])\n",
    "hidden_tensor = tf.concat(1, [prior_tensor, tgi_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoder net is 166+1->128->64->3+1\n",
    "enc_w = {}\n",
    "eff_w = {}\n",
    "enc_w['w1'] = tf.Variable(tf.random_normal([input_space+1, 128]))\n",
    "enc_w['w2'] = tf.Variable(tf.random_normal([128, 64]))\n",
    "enc_w['w3'] = tf.Variable(tf.random_normal([64, latent_space]))\n",
    "enc_w['b1'] = tf.Variable(tf.random_normal([128]))\n",
    "enc_w['b2'] = tf.Variable(tf.random_normal([64]))\n",
    "enc_w['b3'] = tf.Variable(tf.random_normal([latent_space]))\n",
    "eff_w['w'] = tf.Variable(tf.random_normal([64, 1]))\n",
    "eff_w['b'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decoder net 3+1->64->128->166+1\n",
    "dec_w = {}\n",
    "dec_w['w1'] = tf.Variable(tf.random_normal([latent_space+1, 64]))\n",
    "dec_w['w2'] = tf.Variable(tf.random_normal([64, 128]))\n",
    "dec_w['w3'] = tf.Variable(tf.random_normal([128, input_space]))\n",
    "dec_w['b1'] = tf.Variable(tf.random_normal([64]))\n",
    "dec_w['b2'] = tf.Variable(tf.random_normal([128]))\n",
    "dec_w['b3'] = tf.Variable(tf.random_normal([input_space]))\n",
    "dec_w['conc_w'] = tf.Variable(tf.random_normal([128, 1]))\n",
    "dec_w['conc_b'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator net 3->64->3->1\n",
    "disc_w = {}\n",
    "disc_w['w1'] = tf.Variable(tf.random_normal([latent_space, 2*latent_space]))\n",
    "#disc_w['w2'] = tf.Variable(tf.random_normal([64, latent_space]))\n",
    "disc_w['w3'] = tf.Variable(tf.random_normal([2*latent_space, 1]))\n",
    "disc_w['b1'] = tf.Variable(tf.random_normal([2*latent_space]))\n",
    "#disc_w['b2'] = tf.Variable(tf.random_normal([latent_space]))\n",
    "disc_w['b3'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_l1 = tf.nn.tanh(tf.add(tf.matmul(visible_tensor, enc_w['w1']), enc_w['b1']))\n",
    "enc_l2 = tf.nn.tanh(tf.add(tf.matmul(enc_l1, enc_w['w2']), enc_w['b2']))\n",
    "encoded_fp = tf.add(tf.matmul(enc_l2, enc_w['w3']), enc_w['b3'])\n",
    "encoded_tgi = tf.add(tf.matmul(enc_l2, eff_w['w']), eff_w['b'])\n",
    "\n",
    "encoded = tf.concat(1, [encoded_fp, encoded_tgi])\n",
    "\n",
    "dec_l1 = tf.nn.tanh(tf.add(tf.matmul(encoded, dec_w['w1']), dec_w['b1']))\n",
    "dec_l2 = tf.nn.tanh(tf.add(tf.matmul(dec_l1, dec_w['w2']), dec_w['b2']))\n",
    "decoded_fp = tf.add(tf.matmul(dec_l2, dec_w['w3']), dec_w['b3'])\n",
    "decoded_conc = tf.add(tf.matmul(dec_l2, dec_w['conc_w']), dec_w['conc_b'])\n",
    "\n",
    "disc_enc_l1 = tf.nn.sigmoid(tf.add(tf.matmul(encoded_fp, disc_w['w1']), disc_w['b1']))\n",
    "#disc_enc_l2 = tf.nn.sigmoid(tf.add(tf.matmul(disc_enc_l1, disc_w['w2']), disc_w['b2']))\n",
    "disc_enc = tf.add(tf.matmul(disc_enc_l1, disc_w['w3']), disc_w['b3'])\n",
    "\n",
    "disc_prior_l1 = tf.nn.sigmoid(tf.add(tf.matmul(prior_tensor, disc_w['w1']), disc_w['b1']))\n",
    "#disc_prior_l2 = tf.nn.sigmoid(tf.add(tf.matmul(disc_prior_l1, disc_w['w2']), disc_w['b2']))\n",
    "disc_prior = tf.add(tf.matmul(disc_prior_l1, disc_w['w3']), disc_w['b3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_l1 = tf.nn.tanh(tf.add(tf.matmul(hidden_tensor, dec_w['w1']), dec_w['b1']))\n",
    "gen_l2 = tf.nn.tanh(tf.add(tf.matmul(gen_l1, dec_w['w2']), dec_w['b2']))\n",
    "gen_fp = tf.nn.sigmoid(tf.add(tf.matmul(gen_l2, dec_w['w3']), dec_w['b3']))\n",
    "gen_conc = tf.add(tf.matmul(gen_l2, dec_w['conc_w']), dec_w['conc_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_loss = tf.reduce_mean(tf.nn.relu(disc_prior) - disc_prior + tf.log(1.0 + tf.exp(-tf.abs(disc_prior)))) + \\\n",
    "            tf.reduce_mean(tf.nn.relu(disc_enc) + tf.log(1.0 + tf.exp(-tf.abs(disc_enc))))\n",
    "\n",
    "fp_norms = tf.sqrt(tf.reduce_sum(tf.square(encoded_fp), keep_dims=True, reduction_indices=[1]))\n",
    "normalized_fp = tf.div(encoded_fp, fp_norms)\n",
    "cosines_fp = tf.matmul(normalized_fp, tf.transpose(normalized_fp))\n",
    "manifold_cost = tf.reduce_mean(1 - tf.boolean_mask(cosines_fp, targets_tensor))\n",
    "    \n",
    "enc_fp_loss = tf.reduce_mean(tf.nn.relu(disc_enc) - disc_enc + tf.log(1.0 + tf.exp(-tf.abs(disc_enc))))\n",
    "enc_tgi_loss = tf.reduce_mean(tf.square(tf.sub(tgi_tensor, encoded_tgi)))\n",
    "enc_loss = enc_fp_loss\n",
    "\n",
    "dec_fp_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(decoded_fp, fingerprint_tensor))\n",
    "dec_conc_loss = tf.reduce_mean(tf.square(tf.sub(conc_tensor, decoded_conc)))\n",
    "dec_loss = dec_fp_loss + dec_conc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# train_discriminator = tf.train.AdagradOptimizer(learning_rate).minimize(disc_loss, var_list=disc_w.values())\n",
    "# train_encoder = tf.train.AdagradOptimizer(learning_rate).minimize(enc_loss, var_list=enc_w.values())\n",
    "# train_manifold = tf.train.AdagradOptimizer(learning_rate).minimize(manifold_cost, var_list=enc_w.values())\n",
    "# train_reg = tf.train.AdagradOptimizer(learning_rate).minimize(enc_tgi_loss, var_list=enc_w.values()+eff_w.values())\n",
    "# train_autoencoder = tf.train.AdagradOptimizer(learning_rate).minimize(dec_loss, var_list=enc_w.values()+eff_w.values()+dec_w.values())\n",
    "\n",
    "train_discriminator = tf.train.AdamOptimizer(learning_rate).minimize(disc_loss, var_list=disc_w.values())\n",
    "train_encoder = tf.train.AdamOptimizer(learning_rate).minimize(enc_loss, var_list=enc_w.values())\n",
    "train_manifold = tf.train.AdamOptimizer(learning_rate).minimize(manifold_cost, var_list=enc_w.values())\n",
    "train_reg = tf.train.AdamOptimizer(learning_rate).minimize(enc_tgi_loss, var_list=enc_w.values()+eff_w.values())\n",
    "train_autoencoder = tf.train.AdamOptimizer(learning_rate).minimize(dec_loss, var_list=enc_w.values()+eff_w.values()+dec_w.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = batch_gen(train_data, pretrain_batch_size)\n",
    "sames = same_gen(unique_fp, n_different=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "disc: 0.190337, enc_p: 2.408769, mani: 0.002079\n",
      "epoch #1\n",
      "disc: 0.016269, enc_p: 4.458658, mani: 0.000000\n",
      "epoch #2\n",
      "disc: 0.003061, enc_p: 5.737958, mani: -0.000000\n",
      "epoch #3\n",
      "disc: 0.001393, enc_p: 6.522139, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.000729, enc_p: 7.165398, mani: 0.000000\n",
      "epoch #5\n",
      "disc: 0.000407, enc_p: 7.746550, mani: 0.000000\n",
      "epoch #6\n",
      "disc: 0.000235, enc_p: 8.294210, mani: -0.000000\n",
      "epoch #7\n",
      "disc: 0.000138, enc_p: 8.824687, mani: 0.000000\n",
      "epoch #8\n",
      "disc: 0.000082, enc_p: 9.345799, mani: -0.000000\n",
      "epoch #9\n",
      "disc: 0.000049, enc_p: 9.862336, mani: 0.000000\n",
      "epoch #10\n",
      "disc: 0.000029, enc_p: 10.380089, mani: 0.000000\n",
      "epoch #11\n",
      "disc: 0.000017, enc_p: 10.903279, mani: 0.000000\n",
      "epoch #12\n",
      "disc: 0.000010, enc_p: 11.438785, mani: 0.000000\n",
      "epoch #13\n",
      "disc: 0.000006, enc_p: 11.993212, mani: -0.000000\n",
      "epoch #14\n",
      "disc: 0.000005, enc_p: 12.083346, mani: -0.000000\n",
      "epoch #0\n",
      "disc: 0.293447, enc_p: 1.983980, mani: 0.001609\n",
      "epoch #1\n",
      "disc: 0.010665, enc_p: 4.690299, mani: 0.000000\n",
      "epoch #2\n",
      "disc: 0.003113, enc_p: 5.851533, mani: 0.000000\n",
      "epoch #3\n",
      "disc: 0.001383, enc_p: 6.653517, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.000717, enc_p: 7.309593, mani: 0.000000\n",
      "epoch #5\n",
      "disc: 0.000399, enc_p: 7.895387, mani: 0.000000\n",
      "epoch #6\n",
      "disc: 0.000231, enc_p: 8.442604, mani: -0.000000\n",
      "epoch #7\n",
      "disc: 0.000137, enc_p: 8.969608, mani: -0.000000\n",
      "epoch #8\n",
      "disc: 0.000082, enc_p: 9.484909, mani: 0.000000\n",
      "epoch #9\n",
      "disc: 0.000049, enc_p: 9.994427, mani: 0.000000\n",
      "epoch #10\n",
      "disc: 0.026843, enc_p: 5.100333, mani: 0.000017\n",
      "epoch #11\n",
      "disc: 0.007236, enc_p: 4.953821, mani: -0.000000\n",
      "epoch #12\n",
      "disc: 0.003934, enc_p: 5.627600, mani: 0.000000\n",
      "epoch #13\n",
      "disc: 0.002159, enc_p: 6.222947, mani: 0.000000\n",
      "epoch #14\n",
      "disc: 0.001243, enc_p: 6.773176, mani: 0.000000\n",
      "epoch #0\n",
      "disc: 0.286019, enc_p: 1.836242, mani: 0.002428\n",
      "epoch #1\n",
      "disc: 0.012796, enc_p: 4.339803, mani: 0.000001\n",
      "epoch #2\n",
      "disc: 0.002858, enc_p: 5.700456, mani: 0.000000\n",
      "epoch #3\n",
      "disc: 0.001246, enc_p: 6.522593, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.047861, enc_p: 5.594278, mani: 0.000074\n",
      "epoch #5\n",
      "disc: 0.011607, enc_p: 4.801181, mani: 0.000000\n",
      "epoch #6\n",
      "disc: 0.002831, enc_p: 6.055559, mani: 0.000000\n",
      "epoch #7\n",
      "disc: 0.001298, enc_p: 6.818442, mani: -0.000000\n",
      "epoch #8\n",
      "disc: 0.000688, enc_p: 7.447074, mani: -0.000000\n",
      "epoch #9\n",
      "disc: 0.000389, enc_p: 8.016365, mani: 0.000000\n",
      "epoch #10\n",
      "disc: 0.000226, enc_p: 8.554797, mani: 0.000000\n",
      "epoch #11\n",
      "disc: 0.000134, enc_p: 9.076470, mani: 0.000000\n",
      "epoch #12\n",
      "disc: 0.000080, enc_p: 9.588782, mani: 0.000000\n",
      "epoch #13\n",
      "disc: 0.000048, enc_p: 10.095375, mani: 0.000000\n",
      "epoch #14\n",
      "disc: 0.000029, enc_p: 10.598535, mani: 0.000000\n",
      "epoch #0\n",
      "disc: 0.276172, enc_p: 1.859174, mani: 0.002026\n",
      "epoch #1\n",
      "disc: 0.016246, enc_p: 4.315716, mani: 0.000000\n",
      "epoch #2\n",
      "disc: 0.005040, enc_p: 5.419667, mani: 0.000000\n",
      "epoch #3\n",
      "disc: 0.002198, enc_p: 6.225699, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.001039, enc_p: 6.951183, mani: 0.000000\n",
      "epoch #5\n",
      "disc: 0.000495, enc_p: 7.661522, mani: 0.000000\n",
      "epoch #6\n",
      "disc: 0.000251, enc_p: 8.327390, mani: 0.000000\n",
      "epoch #7\n",
      "disc: 0.000303, enc_p: 8.200719, mani: 0.000000\n",
      "epoch #8\n",
      "disc: 0.000195, enc_p: 8.643698, mani: 0.000000\n",
      "epoch #9\n",
      "disc: 0.000103, enc_p: 9.274708, mani: -0.000000\n",
      "epoch #10\n",
      "disc: 0.000058, enc_p: 9.842792, mani: 0.000000\n",
      "epoch #11\n",
      "disc: 0.000034, enc_p: 10.380236, mani: 0.000000\n",
      "epoch #12\n",
      "disc: 0.000020, enc_p: 10.901098, mani: -0.000000\n",
      "epoch #13\n",
      "disc: 0.000012, enc_p: 11.412740, mani: -0.000000\n",
      "epoch #14\n",
      "disc: 0.000007, enc_p: 11.918990, mani: -0.000000\n",
      "epoch #0\n",
      "disc: 0.216959, enc_p: 2.285428, mani: 0.002853\n",
      "epoch #1\n",
      "disc: 0.687449, enc_p: 0.714278, mani: 0.000006\n",
      "epoch #2\n",
      "disc: 0.696399, enc_p: 0.691798, mani: -0.000000\n",
      "epoch #3\n",
      "disc: 0.694729, enc_p: 0.693322, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.694156, enc_p: 0.692414, mani: 0.000000\n",
      "epoch #5\n",
      "disc: 0.693401, enc_p: 0.693016, mani: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# pretrain generator w/o regressions and decoding\n",
    "flag = True\n",
    "while flag:\n",
    "    # need to do a few initialization tries, because from some points\n",
    "    # Generator doesn't converge.\n",
    "    \n",
    "    sess.run(init)\n",
    "    for e in xrange(15):\n",
    "        print(\"epoch #%d\" % e)\n",
    "        discriminator_loss = 0.0\n",
    "        encoder_fp_loss = 0.0\n",
    "        mani_loss = 0.0\n",
    "        for u in xrange(1000):\n",
    "            batch_fp, batch_conc, _ = batches.next()\n",
    "            batch_prior = sample_prior()\n",
    "            _, loss = sess.run([train_discriminator, disc_loss], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                            conc_tensor: batch_conc,\n",
    "                                                                            prior_tensor: batch_prior})\n",
    "            discriminator_loss += loss\n",
    "\n",
    "            fp_loss = 2.\n",
    "            count = 0\n",
    "            while fp_loss > 1. and count < 20:\n",
    "                batch_fp, batch_conc, _ = batches.next()\n",
    "                _, fp_loss = sess.run([train_encoder, enc_fp_loss], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                               conc_tensor: batch_conc,})\n",
    "                count += 1\n",
    "            else:\n",
    "                encoder_fp_loss += fp_loss\n",
    "\n",
    "            same_fp, same_conc, targets = sames.next()\n",
    "            _, m_loss = sess.run([train_manifold, manifold_cost], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                             conc_tensor: batch_conc,\n",
    "                                                                             targets_tensor: targets})\n",
    "            mani_loss += m_loss\n",
    "\n",
    "        discriminator_loss /= 1000. * 2.\n",
    "        encoder_fp_loss /= 1000.\n",
    "        mani_loss /= 1000.\n",
    "\n",
    "        print(\"disc: %f, enc_p: %f, mani: %f\" % (discriminator_loss, encoder_fp_loss, mani_loss))\n",
    "        if (e >= 5) and (encoder_fp_loss < 0.7):\n",
    "            flag = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "disc: 0.693160, enc_fp : 0.688109, mani_fp: 0.000000, enc_tgi: 3139.656006, dec_fp : 0.437040, dec_conc : 2.852242\n",
      "epoch #1\n",
      "disc: 0.693346, enc_fp : 0.673428, mani_fp: 0.000000, enc_tgi: 3141.742188, dec_fp : 0.434595, dec_conc : 2.935303\n",
      "epoch #2\n",
      "disc: 0.693147, enc_fp : 0.693253, mani_fp: 0.000000, enc_tgi: 3138.131348, dec_fp : 0.431563, dec_conc : 2.970067\n",
      "epoch #3\n",
      "disc: 0.693147, enc_fp : 0.693231, mani_fp: 0.000000, enc_tgi: 3137.025391, dec_fp : 0.434331, dec_conc : 2.848898\n",
      "epoch #4\n",
      "disc: 0.693147, enc_fp : 0.693091, mani_fp: 0.000000, enc_tgi: 3136.597168, dec_fp : 0.437752, dec_conc : 2.942320\n",
      "epoch #5\n",
      "disc: 0.693147, enc_fp : 0.693067, mani_fp: 0.000000, enc_tgi: 3136.181152, dec_fp : 0.439213, dec_conc : 2.848863\n",
      "epoch #6\n",
      "disc: 0.693147, enc_fp : 0.692878, mani_fp: 0.000000, enc_tgi: 3136.729492, dec_fp : 0.434631, dec_conc : 2.874536\n",
      "epoch #7\n",
      "disc: 0.693147, enc_fp : 0.692960, mani_fp: 0.000000, enc_tgi: 3140.440674, dec_fp : 0.435014, dec_conc : 2.847920\n",
      "epoch #8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6a65bd9482c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                  \u001b[0mconc_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_conc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                  \u001b[0mtgi_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_tgi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                  prior_tensor: batch_prior})\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_conc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batches = batch_gen(train_data, batch_size)\n",
    "sames = same_gen(unique_fp, n_different=32)\n",
    "for e in xrange(50):\n",
    "    print(\"epoch #%d\" % e)\n",
    "    \n",
    "    for u in xrange(10000):\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        batch_prior = sample_prior()\n",
    "        sess.run(train_discriminator, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                 conc_tensor: batch_conc,\n",
    "                                                 tgi_tensor: batch_tgi,\n",
    "                                                 prior_tensor: batch_prior})\n",
    "\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "\n",
    "        sess.run(train_encoder, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                           conc_tensor: batch_conc})\n",
    "\n",
    "#         same_fp, same_conc, targets = sames.next()\n",
    "#         sess.run(train_manifold, feed_dict={fingerprint_tensor: batch_fp,\n",
    "#                                             conc_tensor: batch_conc,\n",
    "#                                             targets_tensor: targets})\n",
    "    \n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        sess.run(train_reg, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                       conc_tensor: batch_conc,\n",
    "                                       tgi_tensor: batch_tgi})\n",
    "\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        sess.run(train_autoencoder, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                               conc_tensor: batch_conc,\n",
    "                                               tgi_tensor: batch_tgi})\n",
    "        \n",
    "    else:\n",
    "        batch_prior = sample_prior((100, latent_space))\n",
    "        losses = sess.run([disc_loss, enc_fp_loss, enc_tgi_loss, dec_fp_loss, dec_conc_loss],\n",
    "                          feed_dict={fingerprint_tensor: train_data[:, :-2],\n",
    "                                     conc_tensor: train_data[:, -2:-1],\n",
    "                                     tgi_tensor: train_data[:, -1:],\n",
    "                                     prior_tensor: batch_prior\n",
    "                                    })\n",
    "        \n",
    "        same_fp, same_conc, targets = sames.next()\n",
    "        m_loss = sess.run(manifold_cost, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                    conc_tensor: batch_conc,\n",
    "                                                    targets_tensor: targets})\n",
    "        \n",
    "        discriminator_loss, encoder_fp_loss, encoder_tgi_loss, autoencoder_fp_loss, autoencoder_conc_loss = losses\n",
    "        print(\"disc: %f, enc_fp : %f, mani_fp: %f, enc_tgi: %f, dec_fp : %f, dec_conc : %f\" % (discriminator_loss/2.,\n",
    "                                                                                               encoder_fp_loss,\n",
    "                                                                                               m_loss,\n",
    "                                                                                               encoder_tgi_loss,\n",
    "                                                                                               autoencoder_fp_loss,\n",
    "                                                                                               autoencoder_conc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./4dN.2L.adam.aae.manifold.5e.model.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, './4dN.2L.adam.aae.manifold.5e.model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, './4dN.2L.128.64.aae.manifold.5e.model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior_batch = sample_prior(size=(1000, latent_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_tgi = np.ones((1000, 1)) * 5. + np.random.normal(0., 5., size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_fp, generated_conc = sess.run([gen_fp, gen_conc], \n",
    "                                        feed_dict={prior_tensor: prior_batch, tgi_tensor: batch_tgi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = np.hstack([generated_fp, generated_conc, batch_tgi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./generated.probs.5e.tgi5', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 166)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled = (np.hstack([(generated_fp > 0.5).astype(np.int), generated_conc, batch_tgi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./generated.sample.50e', sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(generated_conc < -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(batch_tgi < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((generated_conc < -5) & (batch_tgi < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
