{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('./mcf7.data.npy')\n",
    "# 166 bits fingerprint, 1 concentration float, 1 TGI float\n",
    "unique_fp = np.load('./mcf7.unique.fp.npy')\n",
    "# there is 6252 unique fingerprints and multiple experiments with each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "test_data, train_data = np.vsplit(data, [100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "latent_space = 4\n",
    "input_space = 166\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_prior(size=(batch_size, latent_space)):\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "def batch_gen(data, batch_size=batch_size):\n",
    "    max_index = data.shape[0]/batch_size\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(data)\n",
    "        for i in xrange(max_index):\n",
    "            yield np.hsplit(data[batch_size*i:batch_size*(i+1)], [-2, -1])\n",
    "\n",
    "def same_gen(unq_fp, n_examples=batch_size, n_different=1, mu=-5.82, std=1.68):\n",
    "    '''\n",
    "    Generator of same fingerprints with different concentraition\n",
    "    '''\n",
    "    \n",
    "    if n_examples % n_different: \n",
    "        raise ValueError('n_examples(%s) must be divisible by n_different(%s)' % (n_examples, n_different))\n",
    "    max_index = unq_fp.shape[0] / n_different\n",
    "    targets = np.zeros((n_examples, n_examples))\n",
    "    block_size = n_examples/n_different\n",
    "    for i in xrange(n_different):\n",
    "        '''blocks of ones for every block of equal fp's'''\n",
    "        targets[i*block_size:(i+1)*block_size, i*block_size:(i+1)*block_size] = 1.\n",
    "    targets = targets > 0\n",
    "    \n",
    "    while 1:\n",
    "        np.random.shuffle(unq_fp)\n",
    "        for i in xrange(max_index):\n",
    "            batch_conc = np.random.normal(mu, std, size=(n_examples, 1))\n",
    "            batch_fp = np.repeat(unq_fp[i*n_different:(i+1)*n_different], [block_size]*n_different, axis=0)\n",
    "            yield batch_fp, batch_conc, targets\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = batch_gen(train_data)\n",
    "sames = same_gen(unique_fp, n_different=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fingerprint_tensor = tf.placeholder(tf.float32, [None, input_space])\n",
    "prior_tensor = tf.placeholder(tf.float32, [None, latent_space])\n",
    "conc_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "tgi_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "targets_tensor = tf.placeholder(tf.bool, [None, None])\n",
    "\n",
    "visible_tensor = tf.concat(1, [fingerprint_tensor, conc_tensor])\n",
    "hidden_tensor = tf.concat(1, [prior_tensor, tgi_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoder net is 166+1->128->64->3+1\n",
    "enc_w = {}\n",
    "eff_w = {}\n",
    "enc_w['w1'] = tf.Variable(tf.random_normal([input_space+1, 128]))\n",
    "enc_w['w2'] = tf.Variable(tf.random_normal([128, 64]))\n",
    "enc_w['w3'] = tf.Variable(tf.random_normal([64, latent_space]))\n",
    "enc_w['b1'] = tf.Variable(tf.random_normal([128]))\n",
    "enc_w['b2'] = tf.Variable(tf.random_normal([64]))\n",
    "enc_w['b3'] = tf.Variable(tf.random_normal([latent_space]))\n",
    "eff_w['w'] = tf.Variable(tf.random_normal([64, 1]))\n",
    "eff_w['b'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decoder net 3+1->64->128->166+1\n",
    "dec_w = {}\n",
    "dec_w['w1'] = tf.Variable(tf.random_normal([latent_space+1, 64]))\n",
    "dec_w['w2'] = tf.Variable(tf.random_normal([64, 128]))\n",
    "dec_w['w3'] = tf.Variable(tf.random_normal([128, input_space]))\n",
    "dec_w['b1'] = tf.Variable(tf.random_normal([64]))\n",
    "dec_w['b2'] = tf.Variable(tf.random_normal([128]))\n",
    "dec_w['b3'] = tf.Variable(tf.random_normal([input_space]))\n",
    "dec_w['conc_w'] = tf.Variable(tf.random_normal([128, 1]))\n",
    "dec_w['conc_b'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator net 3->64->3->1\n",
    "disc_w = {}\n",
    "disc_w['w1'] = tf.Variable(tf.random_normal([latent_space, 64]))\n",
    "disc_w['w2'] = tf.Variable(tf.random_normal([64, latent_space]))\n",
    "disc_w['w3'] = tf.Variable(tf.random_normal([latent_space, 1]))\n",
    "disc_w['b1'] = tf.Variable(tf.random_normal([64]))\n",
    "disc_w['b2'] = tf.Variable(tf.random_normal([latent_space]))\n",
    "disc_w['b3'] = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_l1 = tf.nn.tanh(tf.add(tf.matmul(visible_tensor, enc_w['w1']), enc_w['b1']))\n",
    "enc_l2 = tf.nn.tanh(tf.add(tf.matmul(enc_l1, enc_w['w2']), enc_w['b2']))\n",
    "encoded_fp = tf.add(tf.matmul(enc_l2, enc_w['w3']), enc_w['b3'])\n",
    "encoded_tgi = tf.add(tf.matmul(enc_l2, eff_w['w']), eff_w['b'])\n",
    "\n",
    "encoded = tf.concat(1, [encoded_fp, encoded_tgi])\n",
    "\n",
    "dec_l1 = tf.nn.tanh(tf.add(tf.matmul(encoded, dec_w['w1']), dec_w['b1']))\n",
    "dec_l2 = tf.nn.tanh(tf.add(tf.matmul(dec_l1, dec_w['w2']), dec_w['b2']))\n",
    "decoded_fp = tf.add(tf.matmul(dec_l2, dec_w['w3']), dec_w['b3'])\n",
    "decoded_conc = tf.add(tf.matmul(dec_l2, dec_w['conc_w']), dec_w['conc_b'])\n",
    "\n",
    "disc_enc_l1 = tf.nn.sigmoid(tf.add(tf.matmul(encoded_fp, disc_w['w1']), disc_w['b1']))\n",
    "disc_enc_l2 = tf.nn.sigmoid(tf.add(tf.matmul(disc_enc_l1, disc_w['w2']), disc_w['b2']))\n",
    "disc_enc = tf.add(tf.matmul(disc_enc_l2, disc_w['w3']), disc_w['b3'])\n",
    "\n",
    "disc_prior_l1 = tf.nn.sigmoid(tf.add(tf.matmul(prior_tensor, disc_w['w1']), disc_w['b1']))\n",
    "disc_prior_l2 = tf.nn.sigmoid(tf.add(tf.matmul(disc_prior_l1, disc_w['w2']), disc_w['b2']))\n",
    "disc_prior = tf.add(tf.matmul(disc_prior_l2, disc_w['w3']), disc_w['b3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_l1 = tf.nn.tanh(tf.add(tf.matmul(hidden_tensor, dec_w['w1']), dec_w['b1']))\n",
    "gen_l2 = tf.nn.tanh(tf.add(tf.matmul(gen_l1, dec_w['w2']), dec_w['b2']))\n",
    "gen_fp = tf.nn.sigmoid(tf.add(tf.matmul(gen_l2, dec_w['w3']), dec_w['b3']))\n",
    "gen_conc = tf.add(tf.matmul(gen_l2, dec_w['conc_w']), dec_w['conc_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_loss = tf.reduce_mean(tf.nn.relu(disc_prior) - disc_prior + tf.log(1.0 + tf.exp(-tf.abs(disc_prior)))) + \\\n",
    "            tf.reduce_mean(tf.nn.relu(disc_enc) + tf.log(1.0 + tf.exp(-tf.abs(disc_enc))))\n",
    "\n",
    "fp_norms = tf.sqrt(tf.reduce_sum(tf.square(encoded_fp), keep_dims=True, reduction_indices=[1]))\n",
    "normalized_fp = tf.div(encoded_fp, fp_norms)\n",
    "cosines_fp = tf.matmul(normalized_fp, tf.transpose(normalized_fp))\n",
    "manifold_cost = tf.reduce_mean(1 - tf.boolean_mask(cosines_fp, targets_tensor))\n",
    "    \n",
    "enc_fp_loss = tf.reduce_mean(tf.nn.relu(disc_enc) - disc_enc + tf.log(1.0 + tf.exp(-tf.abs(disc_enc))))\n",
    "enc_tgi_loss = tf.reduce_mean(tf.square(tf.sub(tgi_tensor, encoded_tgi)))\n",
    "enc_loss = enc_fp_loss\n",
    "\n",
    "dec_fp_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(decoded_fp, fingerprint_tensor))\n",
    "dec_conc_loss = tf.reduce_mean(tf.square(tf.sub(conc_tensor, decoded_conc)))\n",
    "dec_loss = dec_fp_loss + dec_conc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/tensorflow/python/ops/gradients.py:89: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "train_discriminator = tf.train.AdagradOptimizer(learning_rate).minimize(disc_loss, var_list=disc_w.values())\n",
    "train_encoder = tf.train.AdagradOptimizer(learning_rate).minimize(enc_loss, var_list=enc_w.values())\n",
    "train_manifold = tf.train.AdagradOptimizer(learning_rate).minimize(manifold_cost, var_list=enc_w.values())\n",
    "train_reg = tf.train.AdagradOptimizer(learning_rate).minimize(enc_tgi_loss, var_list=enc_w.values()+eff_w.values())\n",
    "train_autoencoder = tf.train.AdagradOptimizer(learning_rate).minimize(dec_loss, var_list=enc_w.values()+eff_w.values()+dec_w.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "#saver.restore(sess, FLAGS.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "disc: 0.533877, enc_p: 0.904860, mani: 0.177430\n",
      "epoch #1\n",
      "disc: 0.656411, enc_p: 0.720880, mani: 0.101243\n",
      "epoch #2\n",
      "disc: 0.518244, enc_p: 0.919300, mani: 0.076550\n",
      "epoch #3\n",
      "disc: 0.514024, enc_p: 0.895180, mani: 0.060972\n",
      "epoch #4\n",
      "disc: 0.490083, enc_p: 0.949340, mani: 0.050470\n",
      "epoch #5\n",
      "disc: 0.440333, enc_p: 1.096810, mani: 0.046215\n",
      "epoch #6\n",
      "disc: 0.414425, enc_p: 1.130063, mani: 0.035666\n",
      "epoch #7\n",
      "disc: 0.297022, enc_p: 1.462350, mani: 0.022688\n",
      "epoch #8\n",
      "disc: 0.319934, enc_p: 1.305969, mani: 0.028621\n",
      "epoch #9\n",
      "disc: 0.671984, enc_p: 0.791403, mani: 0.096465\n",
      "epoch #10\n",
      "disc: 0.710325, enc_p: 0.728220, mani: 0.029027\n",
      "epoch #11\n",
      "disc: 0.697816, enc_p: 0.706325, mani: 0.013916\n",
      "epoch #12\n",
      "disc: 0.696418, enc_p: 0.692403, mani: 0.011994\n"
     ]
    }
   ],
   "source": [
    "# pretrain generator w/o regressions and decoding\n",
    "for e in xrange(25):\n",
    "    print(\"epoch #%d\" % e)\n",
    "    discriminator_loss = 0.0\n",
    "    encoder_fp_loss = 0.0\n",
    "    mani_loss = 0.0\n",
    "        \n",
    "    for u in xrange(1000):\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        batch_prior = sample_prior()\n",
    "        _, loss = sess.run([train_discriminator, disc_loss], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                        conc_tensor: batch_conc,\n",
    "                                                                        tgi_tensor: batch_tgi,\n",
    "                                                                        prior_tensor: batch_prior})\n",
    "        discriminator_loss += loss\n",
    "\n",
    "        for i in xrange(5):\n",
    "            batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "            _, fp_loss = sess.run([train_encoder, enc_fp_loss], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                     conc_tensor: batch_conc,\n",
    "                                                                     tgi_tensor: batch_tgi})\n",
    "            encoder_fp_loss += fp_loss\n",
    "        \n",
    "        same_fp, same_conc, targets = sames.next()\n",
    "        _, m_loss = sess.run([train_manifold, manifold_cost], feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                                 conc_tensor: batch_conc,\n",
    "                                                                 targets_tensor: targets})\n",
    "        mani_loss += m_loss\n",
    "        \n",
    "    discriminator_loss /= 1000. * 2.\n",
    "    encoder_fp_loss /= 1000. * 5\n",
    "    mani_loss /= 1000.\n",
    "\n",
    "    print(\"disc: %f, enc_p: %f, mani: %f\" % (discriminator_loss, encoder_fp_loss, mani_loss))\n",
    "    \n",
    "    if (e >= 5) and (encoder_fp_loss < 0.7):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "disc: 0.686347, enc_fp : 0.670727, mani_fp: 0.233049, enc_tgi: 2863.896240, dec_fp : 0.639242, dec_conc : 1.950566\n",
      "epoch #1\n",
      "disc: 0.694394, enc_fp : 0.655682, mani_fp: 0.185670, enc_tgi: 2811.734375, dec_fp : 0.544510, dec_conc : 1.855850\n",
      "epoch #2\n",
      "disc: 0.694829, enc_fp : 0.673158, mani_fp: 0.156806, enc_tgi: 2737.711426, dec_fp : 0.533495, dec_conc : 1.608720\n",
      "epoch #3\n",
      "disc: 0.696919, enc_fp : 0.678526, mani_fp: 0.068924, enc_tgi: 2661.663818, dec_fp : 0.588115, dec_conc : 1.937247\n",
      "epoch #4\n",
      "disc: 0.685215, enc_fp : 0.720626, mani_fp: 0.171774, enc_tgi: 2608.927002, dec_fp : 0.558460, dec_conc : 1.687506\n",
      "epoch #5\n",
      "disc: 0.693408, enc_fp : 0.686548, mani_fp: 0.125771, enc_tgi: 2591.607178, dec_fp : 0.510687, dec_conc : 1.814044\n",
      "epoch #6\n",
      "disc: 0.692085, enc_fp : 0.694915, mani_fp: 0.133021, enc_tgi: 2544.810791, dec_fp : 0.513019, dec_conc : 1.699413\n",
      "epoch #7\n",
      "disc: 0.694062, enc_fp : 0.695137, mani_fp: 0.084322, enc_tgi: 2498.333252, dec_fp : 0.536219, dec_conc : 1.616222\n",
      "epoch #8\n",
      "disc: 0.694775, enc_fp : 0.697729, mani_fp: 0.060112, enc_tgi: 2466.240723, dec_fp : 0.526253, dec_conc : 1.685316\n",
      "epoch #9\n",
      "disc: 0.692178, enc_fp : 0.697586, mani_fp: 0.070884, enc_tgi: 2433.417236, dec_fp : 0.522347, dec_conc : 1.706258\n",
      "epoch #10\n",
      "disc: 0.696586, enc_fp : 0.692303, mani_fp: 0.115016, enc_tgi: 2404.593262, dec_fp : 0.523271, dec_conc : 1.701599\n",
      "epoch #11\n",
      "disc: 0.695962, enc_fp : 0.689827, mani_fp: 0.120868, enc_tgi: 2374.144043, dec_fp : 0.514318, dec_conc : 1.553601\n",
      "epoch #12\n",
      "disc: 0.691332, enc_fp : 0.695792, mani_fp: 0.116162, enc_tgi: 2334.970947, dec_fp : 0.510797, dec_conc : 1.511548\n",
      "epoch #13\n",
      "disc: 0.688542, enc_fp : 0.700944, mani_fp: 0.095530, enc_tgi: 2300.152832, dec_fp : 0.509548, dec_conc : 1.599638\n",
      "epoch #14\n",
      "disc: 0.693569, enc_fp : 0.697426, mani_fp: 0.125933, enc_tgi: 2281.266113, dec_fp : 0.499521, dec_conc : 1.563154\n",
      "epoch #15\n",
      "disc: 0.694219, enc_fp : 0.694958, mani_fp: 0.081920, enc_tgi: 2259.518311, dec_fp : 0.492463, dec_conc : 1.520185\n",
      "epoch #16\n",
      "disc: 0.695520, enc_fp : 0.686863, mani_fp: 0.070727, enc_tgi: 2236.664795, dec_fp : 0.514301, dec_conc : 1.613389\n",
      "epoch #17\n",
      "disc: 0.691402, enc_fp : 0.694794, mani_fp: 0.052897, enc_tgi: 2162.796631, dec_fp : 0.572977, dec_conc : 1.662449\n",
      "epoch #18\n",
      "disc: 0.692778, enc_fp : 0.694686, mani_fp: 0.103693, enc_tgi: 2159.573975, dec_fp : 0.542662, dec_conc : 1.550280\n",
      "epoch #19\n",
      "disc: 0.693391, enc_fp : 0.692780, mani_fp: 0.038452, enc_tgi: 2146.018799, dec_fp : 0.535524, dec_conc : 1.583596\n",
      "epoch #20\n",
      "disc: 0.692464, enc_fp : 0.693930, mani_fp: 0.045097, enc_tgi: 2119.951172, dec_fp : 0.536538, dec_conc : 1.595917\n",
      "epoch #21\n",
      "disc: 0.692096, enc_fp : 0.695409, mani_fp: 0.063947, enc_tgi: 2096.396484, dec_fp : 0.543838, dec_conc : 1.600422\n",
      "epoch #22\n",
      "disc: 0.690950, enc_fp : 0.695594, mani_fp: 0.044120, enc_tgi: 2081.954834, dec_fp : 0.538431, dec_conc : 1.524348\n",
      "epoch #23\n",
      "disc: 0.692220, enc_fp : 0.694640, mani_fp: 0.102529, enc_tgi: 2071.886475, dec_fp : 0.527249, dec_conc : 1.515027\n",
      "epoch #24\n",
      "disc: 0.693153, enc_fp : 0.694608, mani_fp: 0.030720, enc_tgi: 2057.409912, dec_fp : 0.518486, dec_conc : 1.540692\n",
      "epoch #25\n",
      "disc: 0.692484, enc_fp : 0.693739, mani_fp: 0.077604, enc_tgi: 2047.739746, dec_fp : 0.510279, dec_conc : 1.453516\n",
      "epoch #26\n",
      "disc: 0.692494, enc_fp : 0.694081, mani_fp: 0.109787, enc_tgi: 2033.512695, dec_fp : 0.505329, dec_conc : 1.487348\n",
      "epoch #27\n",
      "disc: 0.693311, enc_fp : 0.693742, mani_fp: 0.035352, enc_tgi: 2022.373047, dec_fp : 0.497990, dec_conc : 1.424043\n",
      "epoch #28\n",
      "disc: 0.692536, enc_fp : 0.693934, mani_fp: 0.083199, enc_tgi: 2010.132568, dec_fp : 0.493681, dec_conc : 1.418439\n",
      "epoch #29\n",
      "disc: 0.693993, enc_fp : 0.694141, mani_fp: 0.044688, enc_tgi: 1997.027222, dec_fp : 0.494336, dec_conc : 1.430057\n",
      "epoch #30\n",
      "disc: 0.693766, enc_fp : 0.693859, mani_fp: 0.046960, enc_tgi: 1986.606323, dec_fp : 0.494227, dec_conc : 1.420932\n",
      "epoch #31\n",
      "disc: 0.693660, enc_fp : 0.693533, mani_fp: 0.043356, enc_tgi: 1974.187866, dec_fp : 0.495343, dec_conc : 1.431495\n",
      "epoch #32\n",
      "disc: 0.695059, enc_fp : 0.693352, mani_fp: 0.077966, enc_tgi: 1959.466431, dec_fp : 0.497231, dec_conc : 1.476950\n",
      "epoch #33\n",
      "disc: 0.692788, enc_fp : 0.692937, mani_fp: 0.033795, enc_tgi: 1947.517578, dec_fp : 0.499052, dec_conc : 1.409513\n",
      "epoch #34\n",
      "disc: 0.692483, enc_fp : 0.693040, mani_fp: 0.114621, enc_tgi: 1934.114014, dec_fp : 0.499922, dec_conc : 1.438050\n",
      "epoch #35\n",
      "disc: 0.693793, enc_fp : 0.693109, mani_fp: 0.064060, enc_tgi: 1920.576050, dec_fp : 0.502521, dec_conc : 1.428409\n",
      "epoch #36\n",
      "disc: 0.693025, enc_fp : 0.693075, mani_fp: 0.042079, enc_tgi: 1909.331177, dec_fp : 0.503069, dec_conc : 1.420268\n",
      "epoch #37\n",
      "disc: 0.693385, enc_fp : 0.693225, mani_fp: 0.038742, enc_tgi: 1896.353027, dec_fp : 0.504554, dec_conc : 1.430485\n",
      "epoch #38\n",
      "disc: 0.693963, enc_fp : 0.693214, mani_fp: 0.018712, enc_tgi: 1884.708496, dec_fp : 0.512316, dec_conc : 1.467875\n",
      "epoch #39\n",
      "disc: 0.692862, enc_fp : 0.692960, mani_fp: 0.047986, enc_tgi: 1868.219360, dec_fp : 0.523131, dec_conc : 1.490613\n",
      "epoch #40\n",
      "disc: 0.692949, enc_fp : 0.692230, mani_fp: 0.017696, enc_tgi: 1854.759155, dec_fp : 0.536751, dec_conc : 1.543365\n",
      "epoch #41\n",
      "disc: 0.693691, enc_fp : 0.692201, mani_fp: 0.020427, enc_tgi: 1839.901978, dec_fp : 0.553486, dec_conc : 1.642280\n",
      "epoch #42\n",
      "disc: 0.693283, enc_fp : 0.692283, mani_fp: 0.044749, enc_tgi: 1826.357788, dec_fp : 0.560726, dec_conc : 1.573830\n",
      "epoch #43\n",
      "disc: 0.694186, enc_fp : 0.692611, mani_fp: 0.028334, enc_tgi: 1814.656738, dec_fp : 0.561799, dec_conc : 1.520259\n",
      "epoch #44\n",
      "disc: 0.693587, enc_fp : 0.693093, mani_fp: 0.018242, enc_tgi: 1804.529297, dec_fp : 0.561316, dec_conc : 1.516307\n",
      "epoch #45\n",
      "disc: 0.692850, enc_fp : 0.693620, mani_fp: 0.026806, enc_tgi: 1792.385010, dec_fp : 0.560505, dec_conc : 1.501220\n",
      "epoch #46\n",
      "disc: 0.692769, enc_fp : 0.693816, mani_fp: 0.038551, enc_tgi: 1784.353027, dec_fp : 0.553945, dec_conc : 1.533783\n",
      "epoch #47\n",
      "disc: 0.692625, enc_fp : 0.694103, mani_fp: 0.045591, enc_tgi: 1774.490479, dec_fp : 0.549384, dec_conc : 1.568005\n",
      "epoch #48\n",
      "disc: 0.692201, enc_fp : 0.694429, mani_fp: 0.024108, enc_tgi: 1765.033936, dec_fp : 0.544128, dec_conc : 1.510472\n",
      "epoch #49\n",
      "disc: 0.691918, enc_fp : 0.694298, mani_fp: 0.058971, enc_tgi: 1760.672363, dec_fp : 0.533858, dec_conc : 1.483134\n"
     ]
    }
   ],
   "source": [
    "for e in xrange(50):\n",
    "    print(\"epoch #%d\" % e)\n",
    "    \n",
    "    for u in xrange(1000):\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        batch_prior = sample_prior()\n",
    "        sess.run(train_discriminator, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                 conc_tensor: batch_conc,\n",
    "                                                 tgi_tensor: batch_tgi,\n",
    "                                                 prior_tensor: batch_prior})\n",
    "    \n",
    "        for i in xrange(5):\n",
    "            sess.run(train_encoder, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                               conc_tensor: batch_conc,\n",
    "                                               tgi_tensor: batch_tgi})\n",
    "            \n",
    "        sess.run(train_manifold, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                            conc_tensor: batch_conc,\n",
    "                                            targets_tensor: targets})\n",
    "    \n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        sess.run(train_reg, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                       conc_tensor: batch_conc,\n",
    "                                       tgi_tensor: batch_tgi})\n",
    "\n",
    "        batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "        sess.run(train_autoencoder, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                               conc_tensor: batch_conc,\n",
    "                                               tgi_tensor: batch_tgi})\n",
    "        \n",
    "    else:\n",
    "        batch_prior = sample_prior((100, latent_space))\n",
    "        losses = sess.run([disc_loss, enc_fp_loss, enc_tgi_loss, dec_fp_loss, dec_conc_loss],\n",
    "                          feed_dict={fingerprint_tensor: train_data[:, :-2],\n",
    "                                     conc_tensor: train_data[:, -2:-1],\n",
    "                                     tgi_tensor: train_data[:, -1:],\n",
    "                                     prior_tensor: batch_prior\n",
    "                                    })\n",
    "        \n",
    "        same_fp, same_conc, targets = sames.next()\n",
    "        m_loss = sess.run(manifold_cost, feed_dict={fingerprint_tensor: batch_fp,\n",
    "                                                    conc_tensor: batch_conc,\n",
    "                                                    targets_tensor: targets})\n",
    "        \n",
    "        discriminator_loss, encoder_fp_loss, encoder_tgi_loss, autoencoder_fp_loss, autoencoder_conc_loss = losses\n",
    "        print(\"disc: %f, enc_fp : %f, mani_fp: %f, enc_tgi: %f, dec_fp : %f, dec_conc : %f\" % (discriminator_loss/2.,\n",
    "                                                                                               encoder_fp_loss,\n",
    "                                                                                               m_loss,\n",
    "                                                                                               encoder_tgi_loss,\n",
    "                                                                                               autoencoder_fp_loss,\n",
    "                                                                                               autoencoder_conc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./4dN.2L.128.64.aae.manifold.model.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, './4dN.2L.128.64.aae.manifold.model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior_batch = sample_prior(size=(1000, latent_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_tgi = np.ones((1000, 1)) * -5. + np.random.normal(0., 5., size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_fp, generated_conc = sess.run([gen_fp, gen_conc], \n",
    "                                        feed_dict={prior_tensor: prior_batch, tgi_tensor: batch_tgi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = np.hstack([generated_fp, generated_conc, batch_tgi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./generated.probs', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
