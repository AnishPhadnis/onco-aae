{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
    "    return strip_def\n",
    "\n",
    "\n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "    \n",
    "def get_input_data(data_file_name, unique_data_file_name):\n",
    "    data = np.load(data_file_name)\n",
    "    # 166 bits fingerprint, 1 concentration float, 1 TGI float\n",
    "    unique_fp = np.load(unique_data_file_name)\n",
    "    # there is 6252 unique fingerprints and multiple experiments with each\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    test_data, train_data = np.vsplit(data, [100])\n",
    "\n",
    "    return test_data, train_data, unique_fp\n",
    "\n",
    "def sample_prior(size=(64, 4)):\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "def batch_gen(data, batch_size=64):\n",
    "    max_index = data.shape[0]/batch_size\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(data)\n",
    "        for i in xrange(max_index):\n",
    "            yield np.hsplit(data[batch_size*i:batch_size*(i+1)], [-2, -1])\n",
    "\n",
    "def same_gen(unq_fp, n_examples=64, n_different=1, mu=-5.82, std=1.68):\n",
    "    '''\n",
    "    Generator of same fingerprints with different concentraition\n",
    "    '''\n",
    "    \n",
    "    if n_examples % n_different: \n",
    "        raise ValueError('n_examples(%s) must be divisible by n_different(%s)' % (n_examples, n_different))\n",
    "    max_index = unq_fp.shape[0] / n_different\n",
    "    targets = np.zeros((n_examples, n_examples))\n",
    "    block_size = n_examples/n_different\n",
    "    for i in xrange(n_different):\n",
    "        '''blocks of ones for every block of equal fp's'''\n",
    "        targets[i*block_size:(i+1)*block_size, i*block_size:(i+1)*block_size] = 1.\n",
    "    targets = targets > 0\n",
    "    \n",
    "    while 1:\n",
    "        np.random.shuffle(unq_fp)\n",
    "        for i in xrange(max_index):\n",
    "            batch_conc = np.random.normal(mu, std, size=(n_examples, 1))\n",
    "            batch_fp = np.repeat(unq_fp[i*n_different:(i+1)*n_different], [block_size]*n_different, axis=0)\n",
    "            yield batch_fp, batch_conc, targets\n",
    "\n",
    "            \n",
    "def uniform_initializer(size_1, size_2):\n",
    "    normalized_size = np.sqrt(6) / (np.sqrt(size_1 + size_2))\n",
    "    return tf.random_uniform([size_1, size_2], minval=-normalized_size, maxval=normalized_size)\n",
    "\n",
    "\n",
    "def get_collections_from_scope(scope_name):\n",
    "    return tf.get_collection(tf.GraphKeys.VARIABLES, scope='scope_name')\n",
    "\n",
    "class AAE(object):\n",
    "    def __init__(self, \n",
    "                 pretrain_batch_size=512, \n",
    "                 batch_size=64, \n",
    "                 latent_space=4, \n",
    "                 input_space=166, \n",
    "                 learning_rate=0.01,\n",
    "                 encoder_layers=2, \n",
    "                 decoder_layers=2,\n",
    "                 discriminator_layers=1):\n",
    "\n",
    "        self.pretrain_batch_size = pretrain_batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_space = latent_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.discriminator_layers = discriminator_layers\n",
    "        self.input_space = input_space\n",
    "\n",
    "        self.fingerprint_tensor = tf.placeholder(tf.float32, [None, input_space])\n",
    "        self.prior_tensor = tf.placeholder(tf.float32, [None, latent_space])\n",
    "        self.conc_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.tgi_tensor = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.targets_tensor = tf.placeholder(tf.bool, [None, None])\n",
    "\n",
    "        self.visible_tensor = tf.concat(1, [self.fingerprint_tensor, self.conc_tensor])\n",
    "        self.hidden_tensor = tf.concat(1, [self.prior_tensor, self.tgi_tensor])\n",
    "        \n",
    "        # Encoder net: 166+1->128->64->3+1\n",
    "        \n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            encoder = [self.visible_tensor]\n",
    "            \n",
    "            sizes = [self.input_space + 1, 128, 64, self.latent_space]\n",
    "\n",
    "            for layer_number in xrange(encoder_layers):\n",
    "                with tf.name_scope(\"encoder-%s\" % layer_number):\n",
    "                    w = tf.Variable(uniform_initializer(sizes[layer_number], sizes[layer_number + 1]), name=\"W\")\n",
    "                    b = tf.Variable(tf.random_normal([sizes[layer_number + 1]]), name=\"b\")\n",
    "                    enc_l = tf.nn.relu(tf.add(tf.matmul(encoder[-1], w), b), name=\"enc_l\")\n",
    "                    encoder.append(enc_l)\n",
    "\n",
    "            with tf.name_scope(\"encoder-fp\"):\n",
    "                w = tf.Variable(uniform_initializer(sizes[-2], sizes[-1]), name=\"W\")\n",
    "                b = tf.Variable(tf.random_normal([sizes[-1]]), name=\"b\")\n",
    "                self.encoded_fp = tf.add(tf.matmul(encoder[-1], w), b)\n",
    "\n",
    "        with tf.name_scope(\"encoder-tgi\"):\n",
    "            w = tf.Variable(uniform_initializer(sizes[-2], 1), name=\"W\")\n",
    "            b = tf.Variable(tf.random_normal([1]), name=\"b\")\n",
    "            self.encoded_tgi = tf.add(tf.matmul(encoder[-1], w), b)\n",
    "\n",
    "        self.encoded = tf.concat(1, [self.encoded_fp, self.encoded_tgi])\n",
    "        \n",
    "        # Decoder net: 3+1->64->128->166+1\n",
    "\n",
    "        sizes = [self.latent_space + 1, 64, 128, self.input_space]\n",
    "\n",
    "        with tf.name_scope(\"decoder\"):\n",
    "            decoder = [self.encoded]\n",
    "            generator = [self.hidden_tensor]\n",
    "\n",
    "            for layer_number in xrange(decoder_layers):\n",
    "                with tf.name_scope(\"decoder-%s\" % layer_number):\n",
    "                    w = tf.Variable(uniform_initializer(sizes[layer_number], sizes[layer_number + 1]), name=\"W\")\n",
    "                    b = tf.Variable(tf.random_normal([sizes[layer_number + 1]]), name=\"b\")\n",
    "                    dec_l = tf.nn.tanh(tf.add(tf.matmul(decoder[-1], w), b), name=\"dec_l\")\n",
    "                    gen_l = tf.nn.tanh(tf.add(tf.matmul(generator[-1], w), b), name=\"gen_l\")\n",
    "                    decoder.append(dec_l)\n",
    "                    generator.append(gen_l)\n",
    "\n",
    "            with tf.name_scope(\"decoder-fp\"):\n",
    "                w = tf.Variable(uniform_initializer(sizes[-2], sizes[-1]), name=\"W\")\n",
    "                b = tf.Variable(tf.random_normal([sizes[-1]]), name=\"b\")\n",
    "                self.decoded_fp = tf.add(tf.matmul(decoder[-1], w), b, name=\"decoder_fp\")\n",
    "                self.gen_fp = tf.nn.relu(tf.add(tf.matmul(generator[-1], w), b), name=\"gen_fp\")\n",
    "\n",
    "            with tf.name_scope(\"decoder-conc\"):\n",
    "                w = tf.Variable(uniform_initializer(sizes[-2], 1), name=\"W\")\n",
    "                b = tf.Variable(tf.random_normal([1]), name=\"b\")\n",
    "                self.decoded_conc = tf.add(tf.matmul(decoder[-1], w), b)\n",
    "                self.gen_conc = tf.add(tf.matmul(generator[-1], w), b)\n",
    "\n",
    "        # Discriminator net: 3->64->3->1\n",
    "        with tf.name_scope(\"discriminator\"):\n",
    "            discriminator_enc = [self.encoded_fp]\n",
    "            discriminator_prior = [self.prior_tensor]\n",
    "\n",
    "            sizes = [self.latent_space, 2 * self.latent_space - 2, 1]\n",
    "\n",
    "            for layer_number in xrange(discriminator_layers):\n",
    "                with tf.name_scope(\"discriminator-%s\" % layer_number):\n",
    "                    w = tf.Variable(uniform_initializer(sizes[layer_number], sizes[layer_number + 1]), name=\"W\")\n",
    "                    b = tf.Variable(tf.random_normal([sizes[layer_number + 1]]), name=\"b\")\n",
    "                    disc_enc = tf.nn.relu(tf.add(tf.matmul(discriminator_enc[-1], w), b), name=\"disc_enc\")\n",
    "                    disc_prior = tf.nn.relu(tf.add(tf.matmul(discriminator_prior[-1], w), b), name=\"disc_prior\")\n",
    "                \n",
    "                    discriminator_enc.append(disc_enc)\n",
    "                    discriminator_prior.append(disc_prior)\n",
    "\n",
    "            with tf.name_scope(\"discriminator-final\"):\n",
    "                w = tf.Variable(uniform_initializer(sizes[-2], sizes[-1]), name=\"W\")\n",
    "                b = tf.Variable(tf.random_normal([sizes[-1]]), name=\"b\")\n",
    "                self.disc_enc = tf.add(tf.matmul(discriminator_enc[-1], w), b, name=\"disc_enc\")\n",
    "                self.disc_prior = tf.add(tf.matmul(discriminator_prior[-1], w), b, name=\"disc_prior\")\n",
    "\n",
    "                \n",
    "        self.disc_loss = tf.reduce_mean(tf.nn.relu(self.disc_prior) - self.disc_prior + tf.log(1.0 + tf.exp(-tf.abs(self.disc_prior)))) + \\\n",
    "            tf.reduce_mean(tf.nn.relu(self.disc_enc) + tf.log(1.0 + tf.exp(-tf.abs(self.disc_enc))))\n",
    "\n",
    "        fp_norms = tf.sqrt(tf.reduce_sum(tf.square(self.encoded_fp), keep_dims=True, reduction_indices=[1]))\n",
    "        normalized_fp = tf.div(self.encoded_fp, fp_norms)\n",
    "        cosines_fp = tf.matmul(normalized_fp, tf.transpose(normalized_fp))\n",
    "        self.manifold_cost = tf.reduce_mean(1 - tf.boolean_mask(cosines_fp, self.targets_tensor))\n",
    "            \n",
    "        self.enc_fp_loss = tf.reduce_mean(tf.nn.relu(self.disc_enc) - self.disc_enc + tf.log(1.0 + tf.exp(-tf.abs(self.disc_enc))))\n",
    "        self.enc_tgi_loss = tf.reduce_mean(tf.square(tf.sub(self.tgi_tensor, self.encoded_tgi)))\n",
    "        self.enc_loss = self.enc_fp_loss\n",
    "\n",
    "        self.dec_fp_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.decoded_fp, self.fingerprint_tensor))\n",
    "        self.dec_conc_loss = tf.reduce_mean(tf.square(tf.sub(self.conc_tensor, self.decoded_conc)))\n",
    "        self.dec_loss = self.dec_fp_loss + self.dec_conc_loss\n",
    "\n",
    "        self.train_discriminator = tf.train.AdamOptimizer(self.learning_rate).minimize(self.disc_loss, var_list=tf.get_collection(tf.GraphKeys.VARIABLES, scope='discriminator'))\n",
    "        self.train_encoder = tf.train.AdamOptimizer(self.learning_rate).minimize(self.enc_loss, var_list=tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder'))\n",
    "        self.train_manifold = tf.train.AdamOptimizer(self.learning_rate).minimize(self.manifold_cost, var_list=tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder'))\n",
    "        self.train_reg = tf.train.AdamOptimizer(self.learning_rate).minimize(self.enc_tgi_loss, var_list=tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder') + tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder-tgi'))\n",
    "        self.train_autoencoder = tf.train.AdamOptimizer(self.learning_rate).minimize(self.dec_loss, var_list=tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder') + tf.get_collection(tf.GraphKeys.VARIABLES, scope='encoder-tgi') + tf.get_collection(tf.GraphKeys.VARIABLES, scope='decoder'))\n",
    "        \n",
    "        \n",
    "        self.test_data, self.train_data, self.unique_fp = get_input_data(\"mcf7.data.npy\", \"mcf7.unique.fp.npy\")\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sames = same_gen(self.unique_fp, n_different=32)\n",
    "\n",
    "        self.pretrain(self.train_data, sess, init, sames)\n",
    "        #sess.run(init)\n",
    "        \n",
    "        batches = batch_gen(self.train_data, self.batch_size)\n",
    "        sames = same_gen(self.unique_fp, n_different=32)\n",
    "        for e in xrange(50):\n",
    "            if e > 0 and e % 10 == 0:\n",
    "                saver.save(sess, './uniform.adam.aae.manifold.%de.model.ckpt' % e)\n",
    "            \n",
    "            print(\"epoch #%d\" % e)\n",
    "            \n",
    "            for u in xrange(10000):\n",
    "                batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "                batch_prior = sample_prior()\n",
    "                sess.run(self.train_discriminator, feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                                         self.conc_tensor: batch_conc,\n",
    "                                                         self.tgi_tensor: batch_tgi,\n",
    "                                                         self.prior_tensor: batch_prior})\n",
    "\n",
    "                batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "\n",
    "                sess.run(self.train_encoder, feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                                   self.conc_tensor: batch_conc})\n",
    "\n",
    "        #         same_fp, same_conc, targets = sames.next()\n",
    "        #         sess.run(train_manifold, feed_dict={fingerprint_tensor: batch_fp,\n",
    "        #                                             conc_tensor: batch_conc,\n",
    "        #                                             targets_tensor: targets})\n",
    "            \n",
    "                batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "                sess.run(self.train_reg, feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                               self.conc_tensor: batch_conc,\n",
    "                                               self.tgi_tensor: batch_tgi})\n",
    "\n",
    "                batch_fp, batch_conc, batch_tgi = batches.next()\n",
    "                sess.run(self.train_autoencoder, feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                                       self.conc_tensor: batch_conc,\n",
    "                                                       self.tgi_tensor: batch_tgi})\n",
    "                \n",
    "            else:\n",
    "                batch_prior = sample_prior((100, self.latent_space))\n",
    "                losses = sess.run([self.disc_loss, self.enc_fp_loss, self.enc_tgi_loss, self.dec_fp_loss, self.dec_conc_loss],\n",
    "                                  feed_dict={self.fingerprint_tensor: self.train_data[:, :-2],\n",
    "                                             self.conc_tensor: self.train_data[:, -2:-1],\n",
    "                                             self.tgi_tensor: self.train_data[:, -1:],\n",
    "                                             self.prior_tensor: batch_prior\n",
    "                                            })\n",
    "                \n",
    "                same_fp, same_conc, targets = sames.next()\n",
    "                m_loss = sess.run(self.manifold_cost, feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                                            self.conc_tensor: batch_conc,\n",
    "                                                            self.targets_tensor: targets})\n",
    "                \n",
    "                discriminator_loss, encoder_fp_loss, encoder_tgi_loss, autoencoder_fp_loss, autoencoder_conc_loss = losses\n",
    "                print(\"disc: %f, enc_fp : %f, mani_fp: %f, enc_tgi: %f, dec_fp : %f, dec_conc : %f\" % (discriminator_loss/2.,\n",
    "                                                                                                       encoder_fp_loss,\n",
    "                                                                                                       m_loss,\n",
    "                                                                                                       encoder_tgi_loss,\n",
    "                                                                                                       autoencoder_fp_loss,\n",
    "                                                                                                       autoencoder_conc_loss))\n",
    "\n",
    "    def pretrain(self, train_data, sess, init, sames):\n",
    "        # pretrain generator w/o regressions and decoding\n",
    "\n",
    "        batches = batch_gen(train_data, self.pretrain_batch_size)\n",
    "        \n",
    "        flag = True\n",
    "        while flag:\n",
    "            # need to do a few initialization tries, because from some points\n",
    "            # Generator doesn't converge.\n",
    "            \n",
    "            sess.run(init)\n",
    "            for e in xrange(15):\n",
    "                print(\"epoch #%d\" % e)\n",
    "                discriminator_loss = 0.0\n",
    "                encoder_fp_loss = 0.0\n",
    "                mani_loss = 0.0\n",
    "                for u in xrange(1000):\n",
    "                    batch_fp, batch_conc, _ = batches.next()\n",
    "                    batch_prior = sample_prior()\n",
    "                    _, loss = sess.run([self.train_discriminator, self.disc_loss], \n",
    "                        feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                            self.conc_tensor: batch_conc,\n",
    "                            self.prior_tensor: batch_prior})\n",
    "                    discriminator_loss += loss\n",
    "\n",
    "                    fp_loss = 2.\n",
    "                    count = 0\n",
    "                    while fp_loss > 1. and count < 20:\n",
    "                        batch_fp, batch_conc, _ = batches.next()\n",
    "                        _, fp_loss = sess.run([self.train_encoder, self.enc_fp_loss], \n",
    "                                            feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                                            self.conc_tensor: batch_conc,})\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        encoder_fp_loss += fp_loss\n",
    "\n",
    "                    same_fp, same_conc, targets = sames.next()\n",
    "                    _, m_loss = sess.run([self.train_manifold, self.manifold_cost], \n",
    "                        feed_dict={self.fingerprint_tensor: batch_fp,\n",
    "                        self.conc_tensor: batch_conc,\n",
    "                        self.targets_tensor: targets})\n",
    "                    \n",
    "                    mani_loss += m_loss\n",
    "\n",
    "                discriminator_loss /= 1000. * 2.\n",
    "                encoder_fp_loss /= 1000.\n",
    "                mani_loss /= 1000.\n",
    "\n",
    "                print(\"disc: %f, enc_p: %f, mani: %f\" % (discriminator_loss, encoder_fp_loss, mani_loss))\n",
    "                if (e >= 5) and (encoder_fp_loss < 0.7):\n",
    "                    flag = False\n",
    "                    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "a = AAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "disc: 1.716907, enc_p: 0.708333, mani: 0.000020\n",
      "epoch #1\n",
      "disc: 0.699811, enc_p: 0.687444, mani: 0.000000\n",
      "epoch #2\n",
      "disc: 0.694964, enc_p: 0.691577, mani: 0.000000\n",
      "epoch #3\n",
      "disc: 0.693890, enc_p: 0.692508, mani: 0.000000\n",
      "epoch #4\n",
      "disc: 0.693481, enc_p: 0.692851, mani: 0.000000\n",
      "epoch #5\n",
      "disc: 0.693342, enc_p: 0.692982, mani: 0.000000\n",
      "epoch #0\n",
      "disc: 0.694729, enc_fp : 0.693187, mani_fp: 0.216888, enc_tgi: 1781.151245, dec_fp : 0.434717, dec_conc : 0.996942\n",
      "epoch #1\n",
      "disc: 0.693147, enc_fp : 0.693199, mani_fp: 0.390732, enc_tgi: 1880.022705, dec_fp : 0.433360, dec_conc : 0.747710\n",
      "epoch #2\n",
      "disc: 0.556728, enc_fp : 0.670376, mani_fp: 0.408044, enc_tgi: 2022.729248, dec_fp : 0.439210, dec_conc : 1.245546\n",
      "epoch #3\n",
      "disc: 0.693147, enc_fp : 0.693152, mani_fp: 0.056878, enc_tgi: 1703.691162, dec_fp : 0.437781, dec_conc : 1.338224\n",
      "epoch #4\n",
      "disc: 0.693147, enc_fp : 0.693134, mani_fp: 0.066458, enc_tgi: 1695.136597, dec_fp : 0.434387, dec_conc : 1.375732\n",
      "epoch #5\n",
      "disc: 0.693147, enc_fp : 0.693148, mani_fp: 0.242921, enc_tgi: 1729.215820, dec_fp : 0.431813, dec_conc : 1.286901\n",
      "epoch #6\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
